{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Theoretical Questions\n",
        "\n",
        "---\n",
        "\n",
        "**1. What is K-Nearest Neighbors (KNN) and how does it work?**  \n",
        "-> KNN is a supervised learning algorithm used for classification and regression. It predicts the label of a new data point by looking at the 'K' closest points (neighbors) in the training set. The majority vote (classification) or average (regression) of neighbors determines the output. For example, to classify an email as spam or not, KNN checks the most similar emails.\n",
        "\n",
        "---\n",
        "\n",
        "**2. What is the difference between KNN Classification and KNN Regression?**  \n",
        "-> KNN Classification predicts a **category** (e.g., \"spam\" or \"not spam\") by majority voting among neighbors. KNN Regression predicts a **continuous value** (e.g., house prices) by averaging the neighbors’ values. The fundamental process is similar, but the output type differs.\n",
        "\n",
        "---\n",
        "\n",
        "**3. What is the role of the distance metric in KNN?**  \n",
        "-> The distance metric (like Euclidean or Manhattan distance) determines how 'closeness' is measured between points. The choice of metric impacts which neighbors are considered closest and thus affects prediction accuracy. For example, Euclidean distance works well when features are equally scaled.\n",
        "\n",
        "---\n",
        "\n",
        "**4. What is the Curse of Dimensionality in KNN?**  \n",
        "-> As the number of features (dimensions) increases, distances between points become less meaningful, making KNN less effective. In high dimensions, all points tend to look similarly distant. For instance, in image recognition with thousands of pixels, KNN might struggle without dimensionality reduction.\n",
        "\n",
        "---\n",
        "\n",
        "**5. How can we choose the best value of K in KNN?**  \n",
        "-> We usually use techniques like **cross-validation** to test different values of K and pick the one giving the best validation performance. Typically, a small K is noisy and large K can be too smooth. Odd values are often preferred to avoid ties.\n",
        "\n",
        "---\n",
        "\n",
        "**6. What are KD Tree and Ball Tree in KNN?**  \n",
        "-> KD Tree and Ball Tree are data structures that partition the data to make neighbor searches faster. KD Tree splits data based on axis-aligned cuts; Ball Tree groups data into hyperspheres. These trees speed up finding nearest neighbors, especially for large datasets.\n",
        "\n",
        "---\n",
        "\n",
        "**7. When should you use KD Tree vs. Ball Tree?**  \n",
        "-> KD Trees are efficient for low-dimensional data (up to ~20 features), while Ball Trees perform better with higher-dimensional datasets. For example, for a 2D geographic dataset, a KD Tree works well; for high-dimensional image data, Ball Tree is preferred.\n",
        "\n",
        "---\n",
        "\n",
        "**8. What are the disadvantages of KNN?**  \n",
        "->  \n",
        "- **Computationally expensive** during prediction (slow with large datasets).  \n",
        "- **Sensitive to irrelevant features** and feature scaling.  \n",
        "- **Curse of Dimensionality** issues.  \n",
        "- **Memory-intensive**, as it needs to store the entire dataset.  \n",
        "Example: KNN can become very slow for millions of records in real-time applications.\n",
        "\n",
        "---\n",
        "\n",
        "**9. How does feature scaling affect KNN?**  \n",
        "-> KNN relies on distance metrics, so features with larger ranges dominate. **Feature scaling** (e.g., Min-Max or Standardization) ensures all features contribute equally to distance calculations. For instance, income (in thousands) and age (in years) need scaling to avoid bias toward income.\n",
        "\n",
        "---\n",
        "\n",
        "**10. What is PCA (Principal Component Analysis)?**  \n",
        "-> PCA is a dimensionality reduction technique that transforms data into fewer dimensions (principal components) while preserving maximum variance. It helps in compressing data without losing significant information, making models faster and sometimes more accurate.\n",
        "\n",
        "---\n",
        "\n",
        "**11. How does PCA work?**  \n",
        "-> PCA identifies the directions (principal components) in which the data varies the most. It computes these directions via eigenvectors and projects data onto them. This way, complex datasets can be simplified while keeping the most important information.\n",
        "\n",
        "---\n",
        "\n",
        "**12. What is the geometric intuition behind PCA?**  \n",
        "-> Geometrically, PCA finds new axes (directions) that best capture data variance. Imagine stretching a cloud of points along a new line where data spreads the most — that line is the first principal component. Subsequent components are orthogonal (perpendicular).\n",
        "\n",
        "---\n",
        "\n",
        "**13. What is the difference between Feature Selection and Feature Extraction?**  \n",
        "->  \n",
        "- **Feature Selection** picks a subset of original features (e.g., dropping irrelevant columns).  \n",
        "- **Feature Extraction** creates new features from existing ones (e.g., PCA generating principal components).  \n",
        "Example: Selecting only 'age' and 'income' vs. combining features into 'spending power.'\n",
        "\n",
        "---\n",
        "\n",
        "**14. What are Eigenvalues and Eigenvectors in PCA?**  \n",
        "-> In PCA, eigenvectors define the directions (principal components) and eigenvalues represent the amount of variance captured along each direction. Larger eigenvalues indicate components capturing more variance. They are key in forming the new reduced feature space.\n",
        "\n",
        "---\n",
        "\n",
        "**15. How do you decide the number of components to keep in PCA?**  \n",
        "-> We choose the number of components that retain a significant percentage of variance (e.g., 95%). A **Scree plot** or **explained variance ratio** helps visualize how many components are enough to capture the majority of information.\n",
        "\n",
        "---\n",
        "\n",
        "**16. Can PCA be used for classification?**  \n",
        "-> Yes, PCA is often used before classification to reduce dimensionality, noise, and training time. However, PCA itself is unsupervised; classification happens after PCA transforms the data. For instance, we can apply PCA to image data before using a classifier.\n",
        "\n",
        "---\n",
        "\n",
        "**17. What are the limitations of PCA?**  \n",
        "->  \n",
        "- Assumes linear relationships.  \n",
        "- Sensitive to outliers.  \n",
        "- Hard to interpret principal components.  \n",
        "- Information loss if too few components are chosen.  \n",
        "Example: Reducing 1000 features to 2 might oversimplify and hurt model performance.\n",
        "\n",
        "---\n",
        "\n",
        "**18. How do KNN and PCA complement each other?**  \n",
        "-> PCA reduces dimensions and noise, making KNN faster and more accurate, especially in high-dimensional data. For instance, using PCA to compress image features before applying KNN speeds up search and improves prediction quality.\n",
        "\n",
        "---\n",
        "\n",
        "**19. How does KNN handle missing values in a dataset?**  \n",
        "-> KNN doesn't inherently handle missing values. Preprocessing steps like **imputation** (filling missing values with mean/median/nearest neighbor estimates) are required. Alternatively, KNN Imputer can estimate missing values based on neighbors' known values.\n",
        "\n",
        "---\n",
        "\n",
        "**20. What are the key differences between PCA and Linear Discriminant Analysis (LDA)?**  \n",
        "->  \n",
        "- PCA is **unsupervised** (ignores labels); LDA is **supervised** (uses class labels).  \n",
        "- PCA maximizes **variance**; LDA maximizes **class separability**.  \n",
        "- PCA is good for compression; LDA is better for classification tasks.  \n",
        "Example: PCA for visualizing data; LDA for building a classifier on reduced data.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KMUfKEVv8yrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Train a KNN Classifier on the Iris dataset and print model accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = knn.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "rN7mSi0c9CLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE)\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create synthetic data\n",
        "X, y = make_regression(n_samples=200, n_features=2, noise=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train KNN Regressor\n",
        "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
        "knn_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = knn_reg.predict(X_test)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "7df9SGyW-zeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy\n",
        "# Euclidean Distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "acc_euclidean = accuracy_score(y_test, knn_euclidean.predict(X_test))\n",
        "\n",
        "# Manhattan Distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "acc_manhattan = accuracy_score(y_test, knn_manhattan.predict(X_test))\n",
        "\n",
        "print(f\"Euclidean Accuracy: {acc_euclidean}\")\n",
        "print(f\"Manhattan Accuracy: {acc_manhattan}\")\n"
      ],
      "metadata": {
        "id": "ueiHHRI4-3x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Train a KNN Classifier with different values of K and visualize decision boundaries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Reduce to 2D\n",
        "X_small, y_small = iris.data[:, :2], iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_small, y_small, test_size=0.3, random_state=42)\n",
        "\n",
        "# Function to plot decision boundaries\n",
        "def plot_decision_boundary(k):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    h = .02\n",
        "    x_min, x_max = X_small[:, 0].min() - 1, X_small[:, 0].max() + 1\n",
        "    y_min, y_max = X_small[:, 1].min() - 1, X_small[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, cmap=ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']))\n",
        "    plt.scatter(X_small[:, 0], X_small[:, 1], c=y_small, edgecolors='k', marker='o')\n",
        "    plt.title(f\"Decision Boundary (k={k})\")\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(3)\n",
        "plot_decision_boundary(7)\n"
      ],
      "metadata": {
        "id": "a3g3Coar-8eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Apply Feature Scaling before training a KNN model and compare results with unscaled data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Without Scaling\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "acc_unscaled = accuracy_score(y_test, knn_unscaled.predict(X_test))\n",
        "\n",
        "# With Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = accuracy_score(y_test, knn_scaled.predict(X_test_scaled))\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_unscaled}\")\n",
        "print(f\"Accuracy with scaling: {acc_scaled}\")\n"
      ],
      "metadata": {
        "id": "SIOTN2IK_DdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26. Train a PCA model on synthetic data and print the explained variance ratio for each component\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, _ = make_regression(n_samples=200, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "id": "rEaefq2g_LHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27. Apply PCA before training a KNN Classifier and compare accuracy with and without PCA\n",
        "# Without PCA\n",
        "knn_no_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_pca.fit(X_train_scaled, y_train)\n",
        "acc_no_pca = accuracy_score(y_test, knn_no_pca.predict(X_test_scaled))\n",
        "\n",
        "# With PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "acc_with_pca = accuracy_score(y_test, knn_pca.predict(X_test_pca))\n",
        "\n",
        "print(f\"Accuracy without PCA: {acc_no_pca}\")\n",
        "print(f\"Accuracy with PCA: {acc_with_pca}\")\n"
      ],
      "metadata": {
        "id": "IctEgYq9_P0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_neighbors': [3,5,7,9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Score:\", grid.best_score_)\n"
      ],
      "metadata": {
        "id": "hxmn7eT__UqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29. Train a KNN Classifier and check the number of misclassified samples\n",
        "# Train\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "# Misclassified samples\n",
        "misclassified = (y_test != y_pred).sum()\n",
        "print(\"Number of misclassified samples:\", misclassified)\n"
      ],
      "metadata": {
        "id": "2u_dpAMi_csP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30. Train a PCA model and visualize the cumulative explained variance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_train_scaled)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Explained Variance vs. Components')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6OtkkQJt_g0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31. Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare accuracy\n",
        "# Uniform Weights\n",
        "knn_uniform = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
        "knn_uniform.fit(X_train_scaled, y_train)\n",
        "acc_uniform = accuracy_score(y_test, knn_uniform.predict(X_test_scaled))\n",
        "\n",
        "# Distance Weights\n",
        "knn_distance = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
        "knn_distance.fit(X_train_scaled, y_train)\n",
        "acc_distance = accuracy_score(y_test, knn_distance.predict(X_test_scaled))\n",
        "\n",
        "print(f\"Uniform Accuracy: {acc_uniform}\")\n",
        "print(f\"Distance Accuracy: {acc_distance}\")\n"
      ],
      "metadata": {
        "id": "y1WxWrUh_lne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#32. Train a KNN Regressor and analyze the effect of different K values on performance\n",
        "k_values = [1,3,5,7,9]\n",
        "mse_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn_reg = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn_reg.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn_reg.predict(X_test_scaled)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "print(\"K values:\", k_values)\n",
        "print(\"MSE scores:\", mse_scores)\n"
      ],
      "metadata": {
        "id": "WmVg77BZ_vAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#33. Implement KNN Imputation for handling missing values in a dataset\n",
        "from sklearn.impute import KNNImputer\n",
        "import numpy as np\n",
        "\n",
        "# Create data with missing values\n",
        "X_missing = X_train_scaled.copy()\n",
        "X_missing[np.random.randint(0, X_missing.shape[0], 10), np.random.randint(0, X_missing.shape[1], 10)] = np.nan\n",
        "\n",
        "# KNN Imputer\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "X_imputed = imputer.fit_transform(X_missing)\n",
        "\n",
        "print(\"Missing values after imputation:\", np.isnan(X_imputed).sum())\n"
      ],
      "metadata": {
        "id": "KEdtXwaR_zEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#34. Train a PCA model and visualize the data projection onto the first two principal components\n",
        "# Apply PCA with 2 components\n",
        "pca_2 = PCA(n_components=2)\n",
        "X_train_pca2 = pca_2.fit_transform(X_train_scaled)\n",
        "X_test_pca2 = pca_2.transform(X_test_scaled)\n",
        "\n",
        "# Plot the data projection\n",
        "plt.scatter(X_train_pca2[:, 0], X_train_pca2[:, 1], c=y_train, cmap='viridis')\n",
        "plt.title(\"Projection onto First Two Principal Components\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dhhdIYxf_3lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#35. Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance\n",
        "# KD Tree Algorithm\n",
        "knn_kd_tree = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\n",
        "knn_kd_tree.fit(X_train_scaled, y_train)\n",
        "acc_kd_tree = accuracy_score(y_test, knn_kd_tree.predict(X_test_scaled))\n",
        "\n",
        "# Ball Tree Algorithm\n",
        "knn_ball_tree = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
        "knn_ball_tree.fit(X_train_scaled, y_train)\n",
        "acc_ball_tree = accuracy_score(y_test, knn_ball_tree.predict(X_test_scaled))\n",
        "\n",
        "print(f\"KD Tree Accuracy: {acc_kd_tree}\")\n",
        "print(f\"Ball Tree Accuracy: {acc_ball_tree}\")\n"
      ],
      "metadata": {
        "id": "dt3-VdSz_8Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#36. Train a PCA model on a high-dimensional dataset and visualize the Scree plot\n",
        "# Create high-dimensional data\n",
        "X_high_dim, _ = make_regression(n_samples=200, n_features=15, noise=0.1, random_state=42)\n",
        "\n",
        "# Apply PCA\n",
        "pca_high_dim = PCA()\n",
        "pca_high_dim.fit(X_high_dim)\n",
        "\n",
        "# Scree Plot\n",
        "plt.plot(range(1, len(pca_high_dim.explained_variance_ratio_)+1),\n",
        "         pca_high_dim.explained_variance_ratio_, marker='o')\n",
        "plt.title(\"Scree Plot\")\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w8wQMUnpAAWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#37. Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Train KNN\n",
        "knn_class = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_class.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = knn_class.predict(X_test_scaled)\n",
        "\n",
        "# Metrics\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n"
      ],
      "metadata": {
        "id": "sdXCldE_AFnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#38. Train a PCA model and analyze the effect of different numbers of components on accuracy\n",
        "components_range = [1, 2, 3, 4, 5]\n",
        "accuracy_by_components = []\n",
        "\n",
        "for n in components_range:\n",
        "    pca = PCA(n_components=n)\n",
        "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "    X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "    knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn_pca.fit(X_train_pca, y_train)\n",
        "    accuracy_by_components.append(accuracy_score(y_test, knn_pca.predict(X_test_pca)))\n",
        "\n",
        "plt.plot(components_range, accuracy_by_components, marker='o')\n",
        "plt.title(\"Effect of Number of Components on Accuracy\")\n",
        "plt.xlabel(\"Number of PCA Components\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vtY42yspAXV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#39. Train a KNN Classifier with different leaf_size values and compare accuracy\n",
        "leaf_sizes = [10, 20, 30, 40, 50]\n",
        "accuracy_by_leaf_size = []\n",
        "\n",
        "for leaf_size in leaf_sizes:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, leaf_size=leaf_size)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    accuracy_by_leaf_size.append(accuracy_score(y_test, knn.predict(X_test_scaled)))\n",
        "\n",
        "plt.plot(leaf_sizes, accuracy_by_leaf_size, marker='o')\n",
        "plt.title(\"Effect of Leaf Size on Accuracy\")\n",
        "plt.xlabel(\"Leaf Size\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UzfRV0b6Abwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#40. Train a PCA model and visualize how data points are transformed before and after PCA\n",
        "# Apply PCA with 2 components\n",
        "pca_2 = PCA(n_components=2)\n",
        "X_train_pca2 = pca_2.fit_transform(X_train_scaled)\n",
        "X_test_pca2 = pca_2.transform(X_test_scaled)\n",
        "\n",
        "# Plot before PCA\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, cmap='viridis')\n",
        "plt.title(\"Before PCA\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "\n",
        "# Plot after PCA\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_train_pca2[:, 0], X_train_pca2[:, 1], c=y_train, cmap='viridis')\n",
        "plt.title(\"After PCA\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HcemRLkwAlnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#41. Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn_wine = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_wine.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = knn_wine.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "R9VV4fUOAqSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#42. Train a KNN Regressor and analyze the effect of different distance metrics on prediction error\n",
        "# Manhattan Distance\n",
        "knn_reg_manhattan = KNeighborsRegressor(n_neighbors=5, metric='manhattan')\n",
        "knn_reg_manhattan.fit(X_train_scaled, y_train)\n",
        "mse_manhattan = mean_squared_error(y_test, knn_reg_manhattan.predict(X_test_scaled))\n",
        "\n",
        "# Euclidean Distance\n",
        "knn_reg_euclidean = KNeighborsRegressor(n_neighbors=5, metric='euclidean')\n",
        "knn_reg_euclidean.fit(X_train_scaled, y_train)\n",
        "mse_euclidean = mean_squared_error(y_test, knn_reg_euclidean.predict(X_test_scaled))\n",
        "\n",
        "print(f\"MSE (Manhattan): {mse_manhattan}\")\n",
        "print(f\"MSE (Euclidean): {mse_euclidean}\")\n"
      ],
      "metadata": {
        "id": "1SxnLqJfAvWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#43. Train a KNN Classifier and evaluate using ROC-AUC score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Train KNN\n",
        "knn_class_roc = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_class_roc.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_prob = knn_class_roc.predict_proba(X_test_scaled)\n",
        "\n",
        "# ROC-AUC Score (multi-class)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')\n",
        "print(f\"ROC-AUC Score: {roc_auc}\")\n"
      ],
      "metadata": {
        "id": "VWIdRduWA0mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#44. Train a PCA model and visualize the variance captured by each principal component\n",
        "# PCA\n",
        "pca_var = PCA()\n",
        "pca_var.fit(X_train_scaled)\n",
        "\n",
        "# Plot variance captured\n",
        "plt.bar(range(1, len(pca_var.explained_variance_ratio_)+1), pca_var.explained_variance_ratio_)\n",
        "plt.title(\"Variance Captured by Each Principal Component\")\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Explained Variance Ratio\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NOgWJh5zA4_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45. Train a KNN Classifier and perform feature selection before training\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Feature selection\n",
        "selector = SelectKBest(f_classif, k=2)\n",
        "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "\n",
        "# Train KNN on selected features\n",
        "knn_selected = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_selected.fit(X_train_selected, y_train)\n",
        "\n",
        "# Evaluate\n",
        "acc_selected = accuracy_score(y_test, knn_selected.predict(X_test_selected))\n",
        "print(f\"Accuracy with Feature Selection: {acc_selected}\")\n",
        ""
      ],
      "metadata": {
        "id": "Xjgyb29FBBfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#46. Train a PCA model and visualize the data reconstruction error after reducing dimensions\n",
        "# Apply PCA\n",
        "pca_reconstruct = PCA(n_components=2)\n",
        "X_train_pca = pca_reconstruct.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca_reconstruct.transform(X_test_scaled)\n",
        "\n",
        "# Inverse transform to get back the original space\n",
        "X_train_reconstructed = pca_reconstruct.inverse_transform(X_train_pca)\n",
        "X_test_reconstructed = pca_reconstruct.inverse_transform(X_test_pca)\n",
        "\n",
        "# Compute reconstruction error\n",
        "train_reconstruction_error = np.mean((X_train_scaled - X_train_reconstructed) ** 2)\n",
        "test_reconstruction_error = np.mean((X_test_scaled - X_test_reconstructed) ** 2)\n",
        "\n",
        "print(f\"Train Reconstruction Error: {train_reconstruction_error}\")\n",
        "print(f\"Test Reconstruction Error: {test_reconstruction_error}\")\n"
      ],
      "metadata": {
        "id": "WwAWhE78BHXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#47. Train a KNN Classifier and visualize the decision boundary\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data[:, :2]  # Take only the first two features for visualization\n",
        "y = data.target\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Create a mesh grid to plot decision boundaries\n",
        "x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                     np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "# Predict on the grid points\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
        "plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, edgecolors='k', marker='o', cmap='coolwarm')\n",
        "plt.title(\"KNN Classifier Decision Boundary\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hsavvrsKC22c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# 48. Train a PCA model and analyze the effect of different numbers of components on data variance\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler # Import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assuming X_train_scaled is not available, we need to load the data and scale it\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the data using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train) # Define and assign X_train_scaled\n",
        "\n",
        "explained_variance_ratios = []\n",
        "n_components_range = range(1, X_train_scaled.shape[1] + 1)\n",
        "\n",
        "for n_components in n_components_range:\n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca.fit(X_train_scaled)\n",
        "    explained_variance_ratios.append(np.sum(pca.explained_variance_ratio_))\n",
        "\n",
        "plt.plot(n_components_range, explained_variance_ratios, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Effect of Number of Components on Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IyQCZZdxCl1d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}